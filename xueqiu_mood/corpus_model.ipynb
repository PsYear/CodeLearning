{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba  \n",
    "\n",
    "def readLines(filename):\n",
    "    # read txt or csv file\n",
    "    fp = open(filename, 'r')\n",
    "    lines = []\n",
    "    for line in fp.readlines():\n",
    "        line = line.strip()\n",
    "        line = line.decode(\"utf-8\")\n",
    "        lines.append(line)\n",
    "    fp.close()\n",
    "    return lines\n",
    "\n",
    "\n",
    "tingyongci_list = []\n",
    "with open(\"tingyonci.txt\", 'r') as filename:\n",
    "    file_list = filename.readlines()\n",
    "    for line in file_list:\n",
    "#         print line[:-1].decode('gbk').encode('utf-8')\n",
    "        line_utf8=line.replace('\\n','').decode('gbk').encode('utf-8')\n",
    "        tingyongci_list.append(unicode(line_utf8,encoding='utf-8'))\n",
    "\n",
    "def parseSent(sentence,tingyongci_list):\n",
    "    pop_list = []\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    for i in seg_list:\n",
    "        if not i in tingyongci_list:\n",
    "             pop_list.append(i) #删除停用词\n",
    "    output = ' '.join(pop_list)  # use space to join them\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import codecs  \n",
    "import os \n",
    "\n",
    "# only content is valid\n",
    "pattern = \"<content>(.*?)</content>\"\n",
    "# csvfile = codecs.open(\"corpus.csv\", 'a', 'utf-8')\n",
    "\n",
    "txfile = open(\"news.dat\",\"r\")\n",
    "lines = txfile.readlines()\n",
    "txfile.close()\n",
    "# for line in lines[0:1000]:\n",
    "#     m = re.match(pattern, line)\n",
    "#     if m:\n",
    "#         print m.group(1)\n",
    "# with open(\"news.dat\", \"r\") as txtfile:\n",
    "#      for line in txtfile:\n",
    "#         m = re.match(pattern, line)\n",
    "#         if m:\n",
    "#             print m.group(1)\n",
    "#             segSent = parseSent(m.group(1))\n",
    "#             csvfile.write(\"%s\" % segSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csvfile = codecs.open(\"corpus.csv\", 'a', 'utf-8')\n",
    "for line in lines:\n",
    "    try:\n",
    "        m = re.match(pattern, line.decode('gbk'))\n",
    "    except:\n",
    "            continue\n",
    "    if m:\n",
    "        segSent = parseSent(m.group(1).encode('utf8'),tingyongci_list)\n",
    "        csvfile.write(\"%s\" % segSent)\n",
    "# print lines[16].decode('gbk')\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-22 23:18:05,139 : INFO : collecting all words and their counts\n",
      "2017-04-22 23:18:05,150 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-22 23:18:15,019 : INFO : collected 336204 word types from a corpus of 16594902 raw words and 1660 sentences\n",
      "2017-04-22 23:18:15,021 : INFO : Loading a fresh vocabulary\n",
      "2017-04-22 23:18:16,516 : INFO : min_count=5 retains 108811 unique words (32% of original 336204, drops 227393)\n",
      "2017-04-22 23:18:16,517 : INFO : min_count=5 leaves 16218139 word corpus (97% of original 16594902, drops 376763)\n",
      "2017-04-22 23:18:16,883 : INFO : deleting the raw counts dictionary of 336204 items\n",
      "2017-04-22 23:18:16,952 : INFO : sample=0.001 downsamples 28 most-common words\n",
      "2017-04-22 23:18:16,956 : INFO : downsampling leaves estimated 14220553 word corpus (87.7% of prior 16218139)\n",
      "2017-04-22 23:18:16,960 : INFO : estimated required memory for 108811 words and 400 dimensions: 402600700 bytes\n",
      "2017-04-22 23:18:17,483 : INFO : resetting layer weights\n",
      "2017-04-22 23:18:19,445 : INFO : training model with 3 workers on 108811 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-04-22 23:18:20,501 : INFO : PROGRESS: at 0.31% examples, 248246 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:21,586 : INFO : PROGRESS: at 0.54% examples, 208519 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:22,588 : INFO : PROGRESS: at 0.90% examples, 235134 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:23,607 : INFO : PROGRESS: at 1.22% examples, 237199 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:24,613 : INFO : PROGRESS: at 1.63% examples, 254526 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:18:25,634 : INFO : PROGRESS: at 1.98% examples, 257811 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:18:26,650 : INFO : PROGRESS: at 2.40% examples, 268932 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:27,644 : INFO : PROGRESS: at 2.81% examples, 275871 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:18:28,661 : INFO : PROGRESS: at 3.19% examples, 279151 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:18:29,736 : INFO : PROGRESS: at 3.57% examples, 279106 words/s, in_qsize 3, out_qsize 2\n",
      "2017-04-22 23:18:30,745 : INFO : PROGRESS: at 3.98% examples, 283380 words/s, in_qsize 3, out_qsize 2\n",
      "2017-04-22 23:18:31,793 : INFO : PROGRESS: at 4.45% examples, 289921 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:32,862 : INFO : PROGRESS: at 4.87% examples, 291887 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:18:33,897 : INFO : PROGRESS: at 5.27% examples, 293049 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:34,903 : INFO : PROGRESS: at 5.70% examples, 296334 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:35,910 : INFO : PROGRESS: at 6.13% examples, 297350 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:36,940 : INFO : PROGRESS: at 6.54% examples, 295648 words/s, in_qsize 5, out_qsize 2\n",
      "2017-04-22 23:18:37,945 : INFO : PROGRESS: at 7.02% examples, 297130 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:18:38,945 : INFO : PROGRESS: at 7.52% examples, 298990 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:39,975 : INFO : PROGRESS: at 8.00% examples, 300248 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:18:40,952 : INFO : PROGRESS: at 8.42% examples, 299411 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:41,967 : INFO : PROGRESS: at 8.89% examples, 300069 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:18:42,976 : INFO : PROGRESS: at 9.37% examples, 301096 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:18:44,002 : INFO : PROGRESS: at 9.87% examples, 302184 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:45,013 : INFO : PROGRESS: at 10.33% examples, 301973 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:46,017 : INFO : PROGRESS: at 10.80% examples, 302393 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:18:47,027 : INFO : PROGRESS: at 11.28% examples, 302935 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:18:48,035 : INFO : PROGRESS: at 11.63% examples, 300243 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:18:49,041 : INFO : PROGRESS: at 12.07% examples, 300042 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:18:50,069 : INFO : PROGRESS: at 12.57% examples, 300864 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:18:51,124 : INFO : PROGRESS: at 13.05% examples, 301054 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:18:52,127 : INFO : PROGRESS: at 13.53% examples, 301746 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:53,142 : INFO : PROGRESS: at 14.01% examples, 302257 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:54,165 : INFO : PROGRESS: at 14.48% examples, 302375 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:18:55,183 : INFO : PROGRESS: at 14.90% examples, 301884 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:56,173 : INFO : PROGRESS: at 15.37% examples, 302274 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:18:57,254 : INFO : PROGRESS: at 15.87% examples, 302375 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:18:58,283 : INFO : PROGRESS: at 16.22% examples, 300382 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:18:59,284 : INFO : PROGRESS: at 16.73% examples, 301539 words/s, in_qsize 4, out_qsize 0\n",
      "2017-04-22 23:19:00,301 : INFO : PROGRESS: at 17.23% examples, 302669 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:01,295 : INFO : PROGRESS: at 17.71% examples, 303178 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:02,312 : INFO : PROGRESS: at 18.28% examples, 304974 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:03,386 : INFO : PROGRESS: at 18.77% examples, 305235 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:19:04,404 : INFO : PROGRESS: at 19.22% examples, 305121 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:05,413 : INFO : PROGRESS: at 19.65% examples, 304627 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-22 23:19:06,418 : INFO : PROGRESS: at 20.12% examples, 305059 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:07,434 : INFO : PROGRESS: at 20.53% examples, 305530 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:08,434 : INFO : PROGRESS: at 20.90% examples, 305434 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:09,448 : INFO : PROGRESS: at 21.30% examples, 305563 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:10,487 : INFO : PROGRESS: at 21.73% examples, 306134 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:11,491 : INFO : PROGRESS: at 22.16% examples, 306738 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:12,509 : INFO : PROGRESS: at 22.57% examples, 307007 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:19:13,523 : INFO : PROGRESS: at 22.98% examples, 307333 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:14,540 : INFO : PROGRESS: at 23.30% examples, 306409 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:19:15,583 : INFO : PROGRESS: at 23.69% examples, 306236 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:16,616 : INFO : PROGRESS: at 24.12% examples, 306812 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:19:17,629 : INFO : PROGRESS: at 24.51% examples, 306797 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-22 23:19:18,680 : INFO : PROGRESS: at 24.93% examples, 307035 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:19,713 : INFO : PROGRESS: at 25.36% examples, 307527 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:20,745 : INFO : PROGRESS: at 25.77% examples, 307672 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:19:21,749 : INFO : PROGRESS: at 26.18% examples, 307363 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:22,766 : INFO : PROGRESS: at 26.66% examples, 307611 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:23,760 : INFO : PROGRESS: at 27.14% examples, 307859 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:24,769 : INFO : PROGRESS: at 27.61% examples, 307968 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:19:25,780 : INFO : PROGRESS: at 28.06% examples, 307809 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:26,780 : INFO : PROGRESS: at 28.53% examples, 307925 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:27,803 : INFO : PROGRESS: at 29.02% examples, 308234 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:28,826 : INFO : PROGRESS: at 29.49% examples, 308321 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:29,832 : INFO : PROGRESS: at 29.93% examples, 307979 words/s, in_qsize 6, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-22 23:19:30,839 : INFO : PROGRESS: at 30.40% examples, 308021 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:31,860 : INFO : PROGRESS: at 30.84% examples, 307802 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:32,878 : INFO : PROGRESS: at 31.33% examples, 307884 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:33,881 : INFO : PROGRESS: at 31.77% examples, 307648 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:34,918 : INFO : PROGRESS: at 32.28% examples, 307929 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:35,964 : INFO : PROGRESS: at 32.73% examples, 307789 words/s, in_qsize 5, out_qsize 2\n",
      "2017-04-22 23:19:36,973 : INFO : PROGRESS: at 33.20% examples, 307826 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:19:37,983 : INFO : PROGRESS: at 33.67% examples, 307896 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:38,996 : INFO : PROGRESS: at 34.17% examples, 308126 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:40,008 : INFO : PROGRESS: at 34.65% examples, 308269 words/s, in_qsize 4, out_qsize 2\n",
      "2017-04-22 23:19:41,070 : INFO : PROGRESS: at 35.01% examples, 307275 words/s, in_qsize 4, out_qsize 2\n",
      "2017-04-22 23:19:42,075 : INFO : PROGRESS: at 35.52% examples, 307650 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:43,087 : INFO : PROGRESS: at 35.99% examples, 307686 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-22 23:19:44,129 : INFO : PROGRESS: at 36.49% examples, 307912 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:45,176 : INFO : PROGRESS: at 36.98% examples, 308095 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:46,177 : INFO : PROGRESS: at 37.43% examples, 308126 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:47,194 : INFO : PROGRESS: at 38.01% examples, 309033 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:48,205 : INFO : PROGRESS: at 38.52% examples, 309356 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:49,219 : INFO : PROGRESS: at 39.00% examples, 309572 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:19:50,222 : INFO : PROGRESS: at 39.47% examples, 309609 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:19:51,249 : INFO : PROGRESS: at 39.94% examples, 309569 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:19:52,275 : INFO : PROGRESS: at 40.33% examples, 309368 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:19:53,281 : INFO : PROGRESS: at 40.73% examples, 309575 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:54,315 : INFO : PROGRESS: at 41.08% examples, 309135 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:19:55,344 : INFO : PROGRESS: at 41.49% examples, 309243 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-22 23:19:56,379 : INFO : PROGRESS: at 41.88% examples, 309128 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:57,384 : INFO : PROGRESS: at 42.27% examples, 309119 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:19:58,406 : INFO : PROGRESS: at 42.69% examples, 309320 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:19:59,439 : INFO : PROGRESS: at 43.11% examples, 309522 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:00,458 : INFO : PROGRESS: at 43.52% examples, 309657 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:01,497 : INFO : PROGRESS: at 43.90% examples, 309548 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:20:02,504 : INFO : PROGRESS: at 44.33% examples, 309806 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:03,505 : INFO : PROGRESS: at 44.73% examples, 309973 words/s, in_qsize 4, out_qsize 0\n",
      "2017-04-22 23:20:04,535 : INFO : PROGRESS: at 45.14% examples, 310060 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:20:05,536 : INFO : PROGRESS: at 45.54% examples, 310123 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:06,565 : INFO : PROGRESS: at 45.99% examples, 310334 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:07,569 : INFO : PROGRESS: at 46.51% examples, 310684 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:08,602 : INFO : PROGRESS: at 47.00% examples, 310795 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:20:09,607 : INFO : PROGRESS: at 47.45% examples, 310693 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:10,620 : INFO : PROGRESS: at 47.92% examples, 310717 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:11,627 : INFO : PROGRESS: at 48.37% examples, 310673 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:12,634 : INFO : PROGRESS: at 48.86% examples, 310793 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:13,638 : INFO : PROGRESS: at 49.31% examples, 310785 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:14,652 : INFO : PROGRESS: at 49.80% examples, 310850 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:15,681 : INFO : PROGRESS: at 50.28% examples, 310859 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:20:16,699 : INFO : PROGRESS: at 50.78% examples, 311047 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:17,726 : INFO : PROGRESS: at 51.22% examples, 310787 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:20:18,754 : INFO : PROGRESS: at 51.71% examples, 310821 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:19,775 : INFO : PROGRESS: at 52.19% examples, 310862 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:20,876 : INFO : PROGRESS: at 52.70% examples, 310844 words/s, in_qsize 5, out_qsize 2\n",
      "2017-04-22 23:20:21,891 : INFO : PROGRESS: at 53.16% examples, 310787 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:22,906 : INFO : PROGRESS: at 53.65% examples, 310928 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:23,947 : INFO : PROGRESS: at 54.16% examples, 311047 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:25,012 : INFO : PROGRESS: at 54.61% examples, 310852 words/s, in_qsize 3, out_qsize 2\n",
      "2017-04-22 23:20:26,028 : INFO : PROGRESS: at 55.06% examples, 310751 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:27,048 : INFO : PROGRESS: at 55.55% examples, 310897 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:28,078 : INFO : PROGRESS: at 56.06% examples, 311009 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:29,113 : INFO : PROGRESS: at 56.55% examples, 311082 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:20:30,118 : INFO : PROGRESS: at 57.04% examples, 311304 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:31,122 : INFO : PROGRESS: at 57.55% examples, 311575 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:32,142 : INFO : PROGRESS: at 58.13% examples, 312150 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:33,152 : INFO : PROGRESS: at 58.61% examples, 312224 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-22 23:20:34,157 : INFO : PROGRESS: at 59.10% examples, 312350 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:35,166 : INFO : PROGRESS: at 59.58% examples, 312397 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:36,188 : INFO : PROGRESS: at 60.05% examples, 312374 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:37,235 : INFO : PROGRESS: at 60.43% examples, 312269 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:38,238 : INFO : PROGRESS: at 60.83% examples, 312324 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:39,257 : INFO : PROGRESS: at 61.25% examples, 312480 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:40,269 : INFO : PROGRESS: at 61.66% examples, 312530 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:41,290 : INFO : PROGRESS: at 62.05% examples, 312461 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:20:42,291 : INFO : PROGRESS: at 62.47% examples, 312638 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:43,303 : INFO : PROGRESS: at 62.89% examples, 312786 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:44,356 : INFO : PROGRESS: at 63.33% examples, 312917 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:45,394 : INFO : PROGRESS: at 63.72% examples, 312881 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-22 23:20:46,398 : INFO : PROGRESS: at 64.12% examples, 312924 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:47,405 : INFO : PROGRESS: at 64.53% examples, 313016 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:48,425 : INFO : PROGRESS: at 64.96% examples, 313225 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:49,416 : INFO : PROGRESS: at 65.35% examples, 313184 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:20:50,443 : INFO : PROGRESS: at 65.76% examples, 313215 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:20:51,477 : INFO : PROGRESS: at 66.23% examples, 313266 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-22 23:20:52,488 : INFO : PROGRESS: at 66.71% examples, 313322 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:53,515 : INFO : PROGRESS: at 67.18% examples, 313288 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:20:54,530 : INFO : PROGRESS: at 67.66% examples, 313341 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:55,541 : INFO : PROGRESS: at 68.13% examples, 313342 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:20:56,566 : INFO : PROGRESS: at 68.63% examples, 313415 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:57,589 : INFO : PROGRESS: at 69.08% examples, 313355 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:20:58,631 : INFO : PROGRESS: at 69.58% examples, 313423 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:20:59,652 : INFO : PROGRESS: at 70.06% examples, 313408 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:00,692 : INFO : PROGRESS: at 70.53% examples, 313330 words/s, in_qsize 4, out_qsize 2\n",
      "2017-04-22 23:21:01,694 : INFO : PROGRESS: at 70.99% examples, 313281 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:02,698 : INFO : PROGRESS: at 71.48% examples, 313356 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-22 23:21:03,717 : INFO : PROGRESS: at 71.98% examples, 313388 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:04,728 : INFO : PROGRESS: at 72.45% examples, 313384 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:05,749 : INFO : PROGRESS: at 72.90% examples, 313310 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:06,752 : INFO : PROGRESS: at 73.37% examples, 313324 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:07,781 : INFO : PROGRESS: at 73.86% examples, 313336 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:21:08,785 : INFO : PROGRESS: at 74.34% examples, 313374 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:09,797 : INFO : PROGRESS: at 74.80% examples, 313329 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:10,806 : INFO : PROGRESS: at 75.27% examples, 313336 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:11,823 : INFO : PROGRESS: at 75.75% examples, 313365 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:12,890 : INFO : PROGRESS: at 76.24% examples, 313385 words/s, in_qsize 5, out_qsize 2\n",
      "2017-04-22 23:21:13,887 : INFO : PROGRESS: at 76.71% examples, 313354 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:14,915 : INFO : PROGRESS: at 77.22% examples, 313555 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:15,932 : INFO : PROGRESS: at 77.71% examples, 313637 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:16,936 : INFO : PROGRESS: at 78.29% examples, 314077 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:17,970 : INFO : PROGRESS: at 78.77% examples, 314111 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:18,986 : INFO : PROGRESS: at 79.27% examples, 314186 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:19,989 : INFO : PROGRESS: at 79.75% examples, 314224 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:20,992 : INFO : PROGRESS: at 80.17% examples, 314147 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:22,048 : INFO : PROGRESS: at 80.57% examples, 314094 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:23,070 : INFO : PROGRESS: at 80.98% examples, 314139 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:24,125 : INFO : PROGRESS: at 81.41% examples, 314237 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-22 23:21:25,152 : INFO : PROGRESS: at 81.82% examples, 314243 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:26,193 : INFO : PROGRESS: at 82.20% examples, 314150 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:27,196 : INFO : PROGRESS: at 82.64% examples, 314312 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:28,202 : INFO : PROGRESS: at 83.05% examples, 314385 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:29,250 : INFO : PROGRESS: at 83.45% examples, 314332 words/s, in_qsize 4, out_qsize 2\n",
      "2017-04-22 23:21:30,253 : INFO : PROGRESS: at 83.86% examples, 314403 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:31,265 : INFO : PROGRESS: at 84.27% examples, 314465 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:32,268 : INFO : PROGRESS: at 84.69% examples, 314578 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:33,351 : INFO : PROGRESS: at 85.06% examples, 314364 words/s, in_qsize 3, out_qsize 2\n",
      "2017-04-22 23:21:34,365 : INFO : PROGRESS: at 85.48% examples, 314456 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:35,384 : INFO : PROGRESS: at 85.90% examples, 314538 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:36,390 : INFO : PROGRESS: at 86.39% examples, 314565 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:37,392 : INFO : PROGRESS: at 86.84% examples, 314534 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:38,394 : INFO : PROGRESS: at 87.30% examples, 314499 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:39,424 : INFO : PROGRESS: at 87.80% examples, 314552 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:40,479 : INFO : PROGRESS: at 88.29% examples, 314554 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:21:41,529 : INFO : PROGRESS: at 88.73% examples, 314411 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:21:42,535 : INFO : PROGRESS: at 89.23% examples, 314503 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:43,569 : INFO : PROGRESS: at 89.73% examples, 314595 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:44,579 : INFO : PROGRESS: at 90.20% examples, 314554 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:45,594 : INFO : PROGRESS: at 90.66% examples, 314489 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:46,609 : INFO : PROGRESS: at 91.14% examples, 314501 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:47,613 : INFO : PROGRESS: at 91.63% examples, 314503 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:48,613 : INFO : PROGRESS: at 92.07% examples, 314416 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:49,638 : INFO : PROGRESS: at 92.53% examples, 314349 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:50,661 : INFO : PROGRESS: at 93.01% examples, 314359 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:51,691 : INFO : PROGRESS: at 93.49% examples, 314365 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:52,717 : INFO : PROGRESS: at 93.96% examples, 314345 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:53,729 : INFO : PROGRESS: at 94.42% examples, 314269 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:21:54,779 : INFO : PROGRESS: at 94.88% examples, 314182 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-22 23:21:55,805 : INFO : PROGRESS: at 95.36% examples, 314192 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:56,815 : INFO : PROGRESS: at 95.87% examples, 314291 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:21:57,828 : INFO : PROGRESS: at 96.33% examples, 314237 words/s, in_qsize 4, out_qsize 0\n",
      "2017-04-22 23:21:58,841 : INFO : PROGRESS: at 96.82% examples, 314300 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:21:59,851 : INFO : PROGRESS: at 97.34% examples, 314513 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:22:00,891 : INFO : PROGRESS: at 97.87% examples, 314657 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-22 23:22:01,903 : INFO : PROGRESS: at 98.37% examples, 314785 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-22 23:22:02,905 : INFO : PROGRESS: at 98.86% examples, 314844 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:22:03,929 : INFO : PROGRESS: at 99.35% examples, 314883 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-22 23:22:04,933 : INFO : PROGRESS: at 99.82% examples, 314873 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-22 23:22:05,256 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-22 23:22:05,275 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-22 23:22:05,293 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-22 23:22:05,295 : INFO : training on 82974510 raw words (71105552 effective words) took 225.8s, 314884 effective words/s\n",
      "2017-04-22 23:22:05,335 : INFO : saving KeyedVectors object under corpus_xueqiu.model, separately None\n",
      "2017-04-22 23:22:05,338 : INFO : not storing attribute syn0norm\n",
      "2017-04-22 23:22:05,345 : INFO : storing np array 'syn0' to corpus_xueqiu.model.syn0.npy\n",
      "2017-04-22 23:22:10,213 : INFO : saved corpus_xueqiu.model\n",
      "2017-04-22 23:22:10,215 : INFO : storing 108811x400 projection weights into corpus_xueqiu.model.bin\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)\n",
    "sentences = word2vec.Text8Corpus(\"corpus.csv\")  # 加载语料\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, size = 400)  # 训练skip-gram模型\n",
    "\n",
    "# 保存模型，以便重用\n",
    "model.wv.save(\"corpus_xueqiu.model\")\n",
    "# 对应的加载方式\n",
    "# model = word2vec.Word2Vec.load(\"corpus.model\")\n",
    "\n",
    "# 以一种C语言可以解析的形式存储词向量\n",
    "model.wv.save_word2vec_format(\"corpus_xueqiu.model.bin\", binary = True)\n",
    "# 对应的加载方式\n",
    "# model = word2vec.Word2Vec.load_word2vec_format(\"corpus.model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-a946c2ad0b3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"corpus.model.bin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.pyc\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                     \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.pyc\u001b[0m in \u001b[0;36madd_word\u001b[0;34m(word, weights)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0;31m# most common scenario: no vocab file given. just make up some bogus counts, in descending order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mword_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0;31m# use count from the vocab file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \"\"\"\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def getWordVecs(wordList):\n",
    "    vecs = []\n",
    "    for word in wordList:\n",
    "        word = word.replace('\\n', '')\n",
    "        try:\n",
    "            # only use the first 500 dimensions as input dimension\n",
    "            vecs.append(model[word])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    # vecs = np.concatenate(vecs)\n",
    "    return np.array(vecs, dtype = 'float')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"corpus.model.bin\", binary=True)\n",
    "\n",
    "import codecs\n",
    "with codecs.open(\"corpus.csv\", 'r', 'utf-8') as filename:\n",
    "    lines = filename.readlines()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#         print getWordVecs(lines[i])\n",
    "        \n",
    "        \n",
    "txfile = open(\"news.dat\",\"r\")\n",
    "lines = txfile.readlines()\n",
    "txfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "临沂（山东），２０１２年６月４日　夫妻“麦客”忙麦收　６月４日，在山东省临沂市郯城县郯城街道米顶村麦田间，范加江驾驶收割机在收获小麦。　三夏时节，山东小麦主产区处处可见“麦客”驾驶联合收割机在麦田中来回穿梭。生活在郯城县郯城街道东风村的范加江、赵琴兰夫妇就是众多“麦客”中的一对。小两口于２０１１年投资１１万多元购买了一台大型小麦联合收割机，成为村里第一对夫妻“麦客”。麦收时节，天一刚亮，夫妻俩就开始为农户收割小麦，中午在田间地头凑合填饱肚子，晚上有时要干到十一、二点。夫妻俩各自分工，丈夫收割，妻子负责量地、看路、买油、替农户装粮袋等。忙的时候，一天能收割６０多亩，一个麦季下来能挣２万多元。　在郯城县，有２００多对夫妻“麦客”驾驶联合收割机忙碌在田间地头。他们辛勤忙碌在麦收一线，为小麦及时归仓挥洒着辛勤的汗水，同时通过劳动也为自己带来了稳定的收入。　新华社发（张春雷　摄）\n",
      "[[ 0.05473173  0.02778099  0.16560383 ...,  0.0167545  -0.09432811\n",
      "  -0.01764592]\n",
      " [-0.41906738  0.20136572 -1.90778852 ..., -1.76687074 -4.19612503\n",
      "   0.32147345]\n",
      " [-0.21894592 -0.40258574  0.36579785 ..., -0.69276565 -1.18439221\n",
      "  -1.09132183]\n",
      " ..., \n",
      " [ 0.31734994 -0.84896654 -1.27452862 ..., -0.00519094  0.07349746\n",
      "  -0.43525115]\n",
      " [ 0.40836939 -1.68012142 -0.93333471 ..., -1.25584495 -0.78605551\n",
      "  -0.12728004]\n",
      " [ 1.02738106 -1.03671229 -1.07190645 ..., -0.48008901 -0.45937032\n",
      "  -0.68130219]]\n"
     ]
    }
   ],
   "source": [
    "a = lines[0:50]\n",
    "\n",
    "def parseSent(sentence):\n",
    "    # use Jieba to parse sentences\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    output = ' '.join(list(seg_list)) # use space to join them\n",
    "    return output\n",
    "\n",
    "pattern = \"<content>(.*?)</content>\"\n",
    "csvfile = codecs.open(\"corpus.csv\", 'w', 'utf-8')\n",
    "for i in a:\n",
    "    try:\n",
    "        m = re.match(pattern, i.decode('gbk'))\n",
    "        print m.group(1)\n",
    "        segSent = parseSent(m.group(1)[0:10].encode('utf8'))\n",
    "        print getWordVecs(segSent)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# for line in a:\n",
    "#     try:\n",
    "#         m = re.match(pattern, line.decode('gbk'))\n",
    "#     except:\n",
    "#             continue\n",
    "#     if m:\n",
    "        \n",
    "#         segSent = parseSent(m.group(1).encode('utf8'))\n",
    "#         print getWordVecs(segSent)\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
