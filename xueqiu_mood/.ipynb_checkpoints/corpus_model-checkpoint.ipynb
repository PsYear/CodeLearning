{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba  \n",
    "\n",
    "def readLines(filename):\n",
    "    # read txt or csv file\n",
    "    fp = open(filename, 'r')\n",
    "    lines = []\n",
    "    for line in fp.readlines():\n",
    "        line = line.strip()\n",
    "        line = line.decode(\"utf-8\")\n",
    "        lines.append(line)\n",
    "    fp.close()\n",
    "    return lines\n",
    "\n",
    "\n",
    "tingyongci_list = []\n",
    "with open(\"tingyonci.txt\", 'r') as filename:\n",
    "    file_list = filename.readlines()\n",
    "    for line in file_list:\n",
    "#         print line[:-1].decode('gbk').encode('utf-8')\n",
    "        line_utf8=line.replace('\\n','').decode('gbk').encode('utf-8')\n",
    "        tingyongci_list.append(unicode(line_utf8,encoding='utf-8'))\n",
    "\n",
    "def parseSent(sentence,tingyongci_list):\n",
    "    pop_list = []\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    for i in seg_list:\n",
    "        if not i in tingyongci_list:\n",
    "             pop_list.append(i) #删除停用词\n",
    "    output = ' '.join(pop_list)  # use space to join them\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re  \n",
    "import codecs  \n",
    "import os \n",
    "\n",
    "# only content is valid\n",
    "pattern = \"<content>(.*?)</content>\"\n",
    "# csvfile = codecs.open(\"corpus.csv\", 'a', 'utf-8')\n",
    "\n",
    "txfile = open(\"news.dat\",\"r\")\n",
    "lines = txfile.readlines()\n",
    "txfile.close()\n",
    "# for line in lines[0:1000]:\n",
    "#     m = re.match(pattern, line)\n",
    "#     if m:\n",
    "#         print m.group(1)\n",
    "# with open(\"news.dat\", \"r\") as txtfile:\n",
    "#      for line in txtfile:\n",
    "#         m = re.match(pattern, line)\n",
    "#         if m:\n",
    "#             print m.group(1)\n",
    "#             segSent = parseSent(m.group(1))\n",
    "#             csvfile.write(\"%s\" % segSent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.724 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "临沂 （ 山东 ） ， ２ ０ １ ２ 年 ６ 月 ４ 日 　 夫妻 “ 麦客 ” 忙 麦收 　 ６ 月 ４ 日 ， 在 山东省 临沂市 郯城县 郯城 街道 米 顶村 麦田 间 ， 范加江 驾驶 收割机 在 收获 小麦 。 　 三夏 时节 ， 山东 小麦 主产区 处处 可见 “ 麦客 ” 驾驶 联合 收割机 在 麦田 中 来回 穿梭 。 生活 在 郯城县 郯城 街道 东风村 的 范加江 、 赵琴兰 夫妇 就是 众多 “ 麦客 ” 中 的 一对 。 小两口 于 ２ ０ １ １ 年 投资 １ １ 万多元 购买 了 一台 大型 小麦 联合 收割机 ， 成为 村里 第一 对 夫妻 “ 麦客 ” 。 麦收 时节 ， 天一 刚亮 ， 夫妻俩 就 开始 为 农户 收割 小麦 ， 中午 在 田间 地头 凑合 填饱 肚子 ， 晚上 有时 要 干到 十一 、 二点 。 夫妻俩 各自 分工 ， 丈夫 收割 ， 妻子 负责 量 地 、 看路 、 买油 、 替 农户 装 粮袋 等 。 忙 的 时候 ， 一天 能 收割 ６ ０ 多亩 ， 一个 麦季 下来 能 挣 ２ 万多元 。 　 在 郯城县 ， 有 ２ ０ ０ 多 对 夫妻 “ 麦客 ” 驾驶 联合 收割机 忙碌 在 田间 地头 。 他们 辛勤 忙碌 在 麦收 一线 ， 为 小麦 及时 归仓 挥洒 着 辛勤 的 汗水 ， 同时 通过 劳动 也 为 自己 带来 了 稳定 的 收入 。 　 新华社 发 （ 张 春雷 　 摄 ）\n"
     ]
    }
   ],
   "source": [
    "csvfile = codecs.open(\"corpus.csv\", 'a', 'utf-8')\n",
    "for line in lines[0:20]:\n",
    "    try:\n",
    "        m = re.match(pattern, line.decode('gbk'))\n",
    "    except:\n",
    "            continue\n",
    "    if m:\n",
    "        segSent = parseSent(m.group(1).encode('utf8'),tingyongci_list)\n",
    "        csvfile.write(\"%s\" % segSent)\n",
    "# print lines[16].decode('gbk')\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-21 09:33:25,775 : INFO : collecting all words and their counts\n",
      "2017-04-21 09:33:25,781 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-04-21 09:33:28,818 : INFO : collected 39582 word types from a corpus of 9910317 raw words and 992 sentences\n",
      "2017-04-21 09:33:28,820 : INFO : Loading a fresh vocabulary\n",
      "2017-04-21 09:33:28,939 : INFO : min_count=5 retains 19647 unique words (49% of original 39582, drops 19935)\n",
      "2017-04-21 09:33:28,941 : INFO : min_count=5 leaves 9876060 word corpus (99% of original 9910317, drops 34257)\n",
      "2017-04-21 09:33:29,012 : INFO : deleting the raw counts dictionary of 39582 items\n",
      "2017-04-21 09:33:29,017 : INFO : sample=0.001 downsamples 17 most-common words\n",
      "2017-04-21 09:33:29,019 : INFO : downsampling leaves estimated 9692439 word corpus (98.1% of prior 9876060)\n",
      "2017-04-21 09:33:29,021 : INFO : estimated required memory for 19647 words and 400 dimensions: 72693900 bytes\n",
      "2017-04-21 09:33:29,134 : INFO : resetting layer weights\n",
      "2017-04-21 09:33:29,429 : INFO : training model with 3 workers on 19647 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-04-21 09:33:30,490 : INFO : PROGRESS: at 0.97% examples, 465575 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:33:31,510 : INFO : PROGRESS: at 1.94% examples, 462443 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:33:32,523 : INFO : PROGRESS: at 2.86% examples, 456927 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:33:33,538 : INFO : PROGRESS: at 3.81% examples, 456172 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:33:34,569 : INFO : PROGRESS: at 4.50% examples, 429699 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:33:35,573 : INFO : PROGRESS: at 5.56% examples, 444037 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:33:36,582 : INFO : PROGRESS: at 6.63% examples, 453854 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:33:37,594 : INFO : PROGRESS: at 7.70% examples, 461617 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:33:38,603 : INFO : PROGRESS: at 8.75% examples, 466663 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:33:39,625 : INFO : PROGRESS: at 9.78% examples, 468497 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:33:40,638 : INFO : PROGRESS: at 10.89% examples, 473851 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:33:41,650 : INFO : PROGRESS: at 11.98% examples, 478023 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:33:42,734 : INFO : PROGRESS: at 13.02% examples, 477567 words/s, in_qsize 4, out_qsize 3\n",
      "2017-04-21 09:33:43,784 : INFO : PROGRESS: at 13.87% examples, 471211 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:33:44,787 : INFO : PROGRESS: at 14.90% examples, 473001 words/s, in_qsize 4, out_qsize 0\n",
      "2017-04-21 09:33:45,792 : INFO : PROGRESS: at 15.97% examples, 475779 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:33:46,808 : INFO : PROGRESS: at 17.02% examples, 477444 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:33:47,838 : INFO : PROGRESS: at 18.10% examples, 479841 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-21 09:33:48,841 : INFO : PROGRESS: at 18.83% examples, 472915 words/s, in_qsize 4, out_qsize 2\n",
      "2017-04-21 09:33:49,852 : INFO : PROGRESS: at 19.68% examples, 468845 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:33:50,855 : INFO : PROGRESS: at 20.56% examples, 466152 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:33:51,873 : INFO : PROGRESS: at 21.53% examples, 465958 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:33:52,888 : INFO : PROGRESS: at 22.54% examples, 466919 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:33:53,897 : INFO : PROGRESS: at 23.43% examples, 465074 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:33:54,939 : INFO : PROGRESS: at 24.38% examples, 464268 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:33:55,960 : INFO : PROGRESS: at 25.12% examples, 460069 words/s, in_qsize 5, out_qsize 2\n",
      "2017-04-21 09:33:56,972 : INFO : PROGRESS: at 26.11% examples, 460532 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-21 09:33:57,999 : INFO : PROGRESS: at 27.08% examples, 460428 words/s, in_qsize 4, out_qsize 2\n",
      "2017-04-21 09:33:59,026 : INFO : PROGRESS: at 27.78% examples, 456156 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:00,050 : INFO : PROGRESS: at 28.47% examples, 451796 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:01,094 : INFO : PROGRESS: at 29.31% examples, 449872 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-21 09:34:02,101 : INFO : PROGRESS: at 30.26% examples, 449974 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:03,106 : INFO : PROGRESS: at 31.31% examples, 451611 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:04,116 : INFO : PROGRESS: at 32.38% examples, 453495 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:05,130 : INFO : PROGRESS: at 33.25% examples, 452445 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:06,156 : INFO : PROGRESS: at 33.77% examples, 446718 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:07,217 : INFO : PROGRESS: at 34.40% examples, 442223 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:34:08,278 : INFO : PROGRESS: at 34.88% examples, 436185 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:34:09,302 : INFO : PROGRESS: at 35.56% examples, 433319 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:34:10,328 : INFO : PROGRESS: at 36.29% examples, 431307 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:11,335 : INFO : PROGRESS: at 37.02% examples, 429247 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:12,353 : INFO : PROGRESS: at 37.48% examples, 424386 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:13,361 : INFO : PROGRESS: at 38.35% examples, 424185 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:14,385 : INFO : PROGRESS: at 39.15% examples, 423241 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:15,387 : INFO : PROGRESS: at 40.02% examples, 422389 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-21 09:34:16,397 : INFO : PROGRESS: at 40.89% examples, 422326 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:17,412 : INFO : PROGRESS: at 41.79% examples, 422680 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-21 09:34:18,438 : INFO : PROGRESS: at 42.72% examples, 422944 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-21 09:34:19,441 : INFO : PROGRESS: at 43.77% examples, 424659 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:20,481 : INFO : PROGRESS: at 44.82% examples, 426037 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:21,511 : INFO : PROGRESS: at 45.79% examples, 426620 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-21 09:34:22,522 : INFO : PROGRESS: at 46.83% examples, 428072 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:23,557 : INFO : PROGRESS: at 47.96% examples, 430168 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:24,620 : INFO : PROGRESS: at 48.93% examples, 430365 words/s, in_qsize 4, out_qsize 5\n",
      "2017-04-21 09:34:25,636 : INFO : PROGRESS: at 50.14% examples, 432952 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-21 09:34:26,645 : INFO : PROGRESS: at 51.23% examples, 434567 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:27,655 : INFO : PROGRESS: at 52.24% examples, 435425 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:28,670 : INFO : PROGRESS: at 53.29% examples, 436662 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:29,667 : INFO : PROGRESS: at 54.33% examples, 437825 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:30,714 : INFO : PROGRESS: at 55.24% examples, 437531 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:31,723 : INFO : PROGRESS: at 56.17% examples, 437723 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:32,727 : INFO : PROGRESS: at 57.24% examples, 439006 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:33,749 : INFO : PROGRESS: at 58.25% examples, 439667 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:34,756 : INFO : PROGRESS: at 59.23% examples, 440216 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:35,768 : INFO : PROGRESS: at 60.24% examples, 440374 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-21 09:34:36,785 : INFO : PROGRESS: at 61.35% examples, 441742 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:37,846 : INFO : PROGRESS: at 62.42% examples, 442498 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:34:38,944 : INFO : PROGRESS: at 62.90% examples, 438887 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:34:39,978 : INFO : PROGRESS: at 63.55% examples, 436886 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-04-21 09:34:40,994 : INFO : PROGRESS: at 64.29% examples, 435795 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:42,005 : INFO : PROGRESS: at 65.30% examples, 436480 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:43,080 : INFO : PROGRESS: at 66.31% examples, 436738 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:44,134 : INFO : PROGRESS: at 66.67% examples, 432932 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:45,148 : INFO : PROGRESS: at 67.32% examples, 431290 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:46,150 : INFO : PROGRESS: at 68.06% examples, 430409 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:34:47,167 : INFO : PROGRESS: at 69.09% examples, 431223 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:48,208 : INFO : PROGRESS: at 69.82% examples, 429958 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:49,297 : INFO : PROGRESS: at 70.26% examples, 426768 words/s, in_qsize 6, out_qsize 2\n",
      "2017-04-21 09:34:50,303 : INFO : PROGRESS: at 71.09% examples, 426400 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:51,321 : INFO : PROGRESS: at 71.77% examples, 425171 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:52,342 : INFO : PROGRESS: at 72.68% examples, 425286 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:53,371 : INFO : PROGRESS: at 73.63% examples, 425552 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:54,402 : INFO : PROGRESS: at 74.58% examples, 425818 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:34:55,420 : INFO : PROGRESS: at 75.60% examples, 426581 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:56,431 : INFO : PROGRESS: at 76.43% examples, 426286 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:34:57,462 : INFO : PROGRESS: at 77.12% examples, 425086 words/s, in_qsize 3, out_qsize 2\n",
      "2017-04-21 09:34:58,524 : INFO : PROGRESS: at 77.88% examples, 424199 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:34:59,566 : INFO : PROGRESS: at 78.63% examples, 423323 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:35:00,586 : INFO : PROGRESS: at 79.07% examples, 420956 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-21 09:35:01,593 : INFO : PROGRESS: at 79.56% examples, 418778 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:35:02,616 : INFO : PROGRESS: at 80.22% examples, 417391 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:35:03,627 : INFO : PROGRESS: at 81.03% examples, 417088 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:35:04,645 : INFO : PROGRESS: at 82.02% examples, 417673 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:35:05,674 : INFO : PROGRESS: at 82.92% examples, 417846 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:35:06,664 : INFO : PROGRESS: at 83.85% examples, 418182 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:35:07,689 : INFO : PROGRESS: at 84.86% examples, 418871 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:35:08,685 : INFO : PROGRESS: at 85.91% examples, 419745 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:35:09,710 : INFO : PROGRESS: at 86.94% examples, 420425 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:35:10,714 : INFO : PROGRESS: at 87.94% examples, 421134 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:35:11,715 : INFO : PROGRESS: at 88.95% examples, 421817 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:35:12,770 : INFO : PROGRESS: at 89.90% examples, 421927 words/s, in_qsize 2, out_qsize 4\n",
      "2017-04-21 09:35:13,800 : INFO : PROGRESS: at 90.73% examples, 421575 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:35:14,825 : INFO : PROGRESS: at 91.53% examples, 421196 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:35:15,833 : INFO : PROGRESS: at 92.32% examples, 420814 words/s, in_qsize 5, out_qsize 1\n",
      "2017-04-21 09:35:16,913 : INFO : PROGRESS: at 93.02% examples, 419794 words/s, in_qsize 5, out_qsize 2\n",
      "2017-04-21 09:35:17,942 : INFO : PROGRESS: at 93.47% examples, 417785 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:35:18,951 : INFO : PROGRESS: at 94.15% examples, 416983 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:35:19,962 : INFO : PROGRESS: at 94.86% examples, 416276 words/s, in_qsize 5, out_qsize 2\n",
      "2017-04-21 09:35:20,971 : INFO : PROGRESS: at 95.44% examples, 415050 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:35:21,976 : INFO : PROGRESS: at 96.05% examples, 413976 words/s, in_qsize 6, out_qsize 1\n",
      "2017-04-21 09:35:22,983 : INFO : PROGRESS: at 96.75% examples, 413341 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:35:24,027 : INFO : PROGRESS: at 97.30% examples, 411881 words/s, in_qsize 5, out_qsize 0\n",
      "2017-04-21 09:35:25,058 : INFO : PROGRESS: at 98.17% examples, 411867 words/s, in_qsize 6, out_qsize 0\n",
      "2017-04-21 09:35:26,143 : INFO : PROGRESS: at 98.77% examples, 410555 words/s, in_qsize 4, out_qsize 1\n",
      "2017-04-21 09:35:27,170 : INFO : PROGRESS: at 99.44% examples, 409650 words/s, in_qsize 3, out_qsize 2\n",
      "2017-04-21 09:35:28,088 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-04-21 09:35:28,124 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-04-21 09:35:28,141 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-04-21 09:35:28,144 : INFO : training on 49551585 raw words (48461238 effective words) took 118.7s, 408362 effective words/s\n",
      "2017-04-21 09:35:28,228 : INFO : saving KeyedVectors object under corpus_xueqiu.model, separately None\n",
      "2017-04-21 09:35:28,230 : INFO : not storing attribute syn0norm\n",
      "2017-04-21 09:35:28,792 : INFO : saved corpus_xueqiu.model\n",
      "2017-04-21 09:35:28,795 : INFO : storing 19647x400 projection weights into corpus_xueqiu.model.bin\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format = '%(asctime)s : %(levelname)s : %(message)s', level = logging.INFO)\n",
    "sentences = word2vec.Text8Corpus(\"corpus.csv\")  # 加载语料\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, size = 400)  # 训练skip-gram模型\n",
    "\n",
    "# 保存模型，以便重用\n",
    "model.wv.save(\"corpus_xueqiu.model\")\n",
    "# 对应的加载方式\n",
    "# model = word2vec.Word2Vec.load(\"corpus.model\")\n",
    "\n",
    "# 以一种C语言可以解析的形式存储词向量\n",
    "model.wv.save_word2vec_format(\"corpus_xueqiu.model.bin\", binary = True)\n",
    "# 对应的加载方式\n",
    "# model = word2vec.Word2Vec.load_word2vec_format(\"corpus.model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-a946c2ad0b3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"corpus.model.bin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.pyc\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfromstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                     \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.pyc\u001b[0m in \u001b[0;36madd_word\u001b[0;34m(word, weights)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0;31m# most common scenario: no vocab file given. just make up some bogus counts, in descending order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mword_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0;31m# use count from the vocab file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/gensim/models/keyedvectors.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \"\"\"\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def getWordVecs(wordList):\n",
    "    vecs = []\n",
    "    for word in wordList:\n",
    "        word = word.replace('\\n', '')\n",
    "        try:\n",
    "            # only use the first 500 dimensions as input dimension\n",
    "            vecs.append(model[word])\n",
    "        except KeyError:\n",
    "            continue\n",
    "    # vecs = np.concatenate(vecs)\n",
    "    return np.array(vecs, dtype = 'float')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"corpus.model.bin\", binary=True)\n",
    "\n",
    "import codecs\n",
    "with codecs.open(\"corpus.csv\", 'r', 'utf-8') as filename:\n",
    "    lines = filename.readlines()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in range(10):\n",
    "#         print getWordVecs(lines[i])\n",
    "        \n",
    "        \n",
    "txfile = open(\"news.dat\",\"r\")\n",
    "lines = txfile.readlines()\n",
    "txfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "临沂（山东），２０１２年６月４日　夫妻“麦客”忙麦收　６月４日，在山东省临沂市郯城县郯城街道米顶村麦田间，范加江驾驶收割机在收获小麦。　三夏时节，山东小麦主产区处处可见“麦客”驾驶联合收割机在麦田中来回穿梭。生活在郯城县郯城街道东风村的范加江、赵琴兰夫妇就是众多“麦客”中的一对。小两口于２０１１年投资１１万多元购买了一台大型小麦联合收割机，成为村里第一对夫妻“麦客”。麦收时节，天一刚亮，夫妻俩就开始为农户收割小麦，中午在田间地头凑合填饱肚子，晚上有时要干到十一、二点。夫妻俩各自分工，丈夫收割，妻子负责量地、看路、买油、替农户装粮袋等。忙的时候，一天能收割６０多亩，一个麦季下来能挣２万多元。　在郯城县，有２００多对夫妻“麦客”驾驶联合收割机忙碌在田间地头。他们辛勤忙碌在麦收一线，为小麦及时归仓挥洒着辛勤的汗水，同时通过劳动也为自己带来了稳定的收入。　新华社发（张春雷　摄）\n",
      "[[ 0.05473173  0.02778099  0.16560383 ...,  0.0167545  -0.09432811\n",
      "  -0.01764592]\n",
      " [-0.41906738  0.20136572 -1.90778852 ..., -1.76687074 -4.19612503\n",
      "   0.32147345]\n",
      " [-0.21894592 -0.40258574  0.36579785 ..., -0.69276565 -1.18439221\n",
      "  -1.09132183]\n",
      " ..., \n",
      " [ 0.31734994 -0.84896654 -1.27452862 ..., -0.00519094  0.07349746\n",
      "  -0.43525115]\n",
      " [ 0.40836939 -1.68012142 -0.93333471 ..., -1.25584495 -0.78605551\n",
      "  -0.12728004]\n",
      " [ 1.02738106 -1.03671229 -1.07190645 ..., -0.48008901 -0.45937032\n",
      "  -0.68130219]]\n"
     ]
    }
   ],
   "source": [
    "a = lines[0:50]\n",
    "\n",
    "def parseSent(sentence):\n",
    "    # use Jieba to parse sentences\n",
    "    seg_list = jieba.cut(sentence)\n",
    "    output = ' '.join(list(seg_list)) # use space to join them\n",
    "    return output\n",
    "\n",
    "pattern = \"<content>(.*?)</content>\"\n",
    "csvfile = codecs.open(\"corpus.csv\", 'w', 'utf-8')\n",
    "for i in a:\n",
    "    try:\n",
    "        m = re.match(pattern, i.decode('gbk'))\n",
    "        print m.group(1)\n",
    "        segSent = parseSent(m.group(1)[0:10].encode('utf8'))\n",
    "        print getWordVecs(segSent)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# for line in a:\n",
    "#     try:\n",
    "#         m = re.match(pattern, line.decode('gbk'))\n",
    "#     except:\n",
    "#             continue\n",
    "#     if m:\n",
    "        \n",
    "#         segSent = parseSent(m.group(1).encode('utf8'))\n",
    "#         print getWordVecs(segSent)\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
